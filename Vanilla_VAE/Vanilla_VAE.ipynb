{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MCsneF2MaoaV"
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "class VAE_Vanilla(nn.Module):\n",
    "  \n",
    "  \"\"\"\n",
    "  Define the VAE_Vanilla class\n",
    "  \n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self,input_size = 784, mid_layer_size = 400, latent_size = 30,output_size=784):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.fc1 = nn.Linear(input_size,mid_layer_size)  #input observed data\n",
    "        self.mu = nn.Linear(mid_layer_size,latent_size)  # mean\n",
    "        self.logvar = nn.Linear(mid_layer_size, latent_size) # variance\n",
    "        self.fc2 = nn.Linear(latent_size, mid_layer_size) # sampled latent variables\n",
    "        self.fc3 = nn.Linear(mid_layer_size,output_size) # reconstruction\n",
    "\n",
    "    \n",
    "    def encoder(self,X):\n",
    "        o1 = F.relu(self.fc1(X))\n",
    "        return self.mu(o1), self.logvar(o1)   # approximate the mean and logarisim of variance\n",
    "\n",
    "    def reparameterize(self, mu,logvar):\n",
    "        \n",
    "        std = torch.exp(0.5*logvar)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mu + epsilon * std\n",
    "\n",
    "    def decoder(self,sampled_latent_z):\n",
    "        o2 = F.relu(self.fc2(sampled_latent_z))\n",
    "        return torch.sigmoid(self.fc3(o2))\n",
    "\n",
    "    def forward(self, X):\n",
    "        mu, logvar = self.encoder(X.view(-1,self.input_size)) # flatten the 2D input here\n",
    "        sampled_latent_z = self.reparameterize(mu,logvar)\n",
    "        return self.decoder(sampled_latent_z), mu, logvar\n",
    "    \n",
    "    def loss_function(self, input_X, output_X, mu,logvar):\n",
    "\n",
    "        reconstruction_loss = - F.binary_cross_entropy(output_X, input_X.view(-1,self.input_size), reduction = 'sum')\n",
    "        \n",
    "        KL_divergence = 0.5 * torch.sum(1+logvar-mu.pow(2)-logvar.exp())\n",
    "        \n",
    "        return - (reconstruction_loss + KL_divergence) # we want to maximize \"reconstruction_loss - KL_divergence\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wwpeey3PbXh3"
   },
   "outputs": [],
   "source": [
    "# define device, model, optimizer, batch_size and import data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VAE_Vanilla().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "BATCH_SIZE= 32\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True,transform=transforms.ToTensor()),batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# Trainning function for each epoch, return the average trainning error\n",
    "def VAE_Vanilla_train():\n",
    "    model.train() # this is for dropout \n",
    "    train_loss = 0\n",
    "\n",
    "    for data, _ in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_data, mu, logvar  = model(data)\n",
    "        loss = model.loss_function(data,recon_data,mu,logvar)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss / len(train_loader.dataset)\n",
    "  \n",
    "\n",
    "# Test function for each epoch, return the average test error    \n",
    "def VAE_Vanilla_test():\n",
    "    model.eval() # this is for dropout removal\n",
    "    test_loss = 0\n",
    "    with torch.no_grad(): # gradient calculation is not needed\n",
    "        for data, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            recon_data, mu, logvar = model(data)\n",
    "            test_loss += model.loss_function(data,recon_data,mu,logvar)\n",
    "\n",
    "    return test_loss/ len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HcMify_SbdSO"
   },
   "outputs": [],
   "source": [
    "!mkdir results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1dgmCQOybEK-",
    "outputId": "64e9859e-0780-45aa-beaf-80de6d93c7cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average Training loss: 139.1330\n",
      "====> Average Test set loss: 116.7939\n",
      "====> Epoch: 2 Average Training loss: 114.1602\n",
      "====> Average Test set loss: 111.0507\n",
      "====> Epoch: 3 Average Training loss: 110.4375\n",
      "====> Average Test set loss: 108.7017\n",
      "====> Epoch: 4 Average Training loss: 108.7540\n",
      "====> Average Test set loss: 107.3010\n",
      "====> Epoch: 5 Average Training loss: 107.8047\n",
      "====> Average Test set loss: 106.6419\n",
      "====> Epoch: 6 Average Training loss: 107.0847\n",
      "====> Average Test set loss: 106.1839\n",
      "====> Epoch: 7 Average Training loss: 106.6227\n",
      "====> Average Test set loss: 105.8181\n",
      "====> Epoch: 8 Average Training loss: 106.1932\n",
      "====> Average Test set loss: 105.7761\n",
      "====> Epoch: 9 Average Training loss: 105.8733\n",
      "====> Average Test set loss: 105.4282\n",
      "====> Epoch: 10 Average Training loss: 105.5985\n",
      "====> Average Test set loss: 105.1616\n",
      "====> Epoch: 11 Average Training loss: 105.3032\n",
      "====> Average Test set loss: 104.6157\n",
      "====> Epoch: 12 Average Training loss: 105.1106\n",
      "====> Average Test set loss: 104.6652\n",
      "====> Epoch: 13 Average Training loss: 104.8895\n",
      "====> Average Test set loss: 104.7738\n",
      "====> Epoch: 14 Average Training loss: 104.7370\n",
      "====> Average Test set loss: 104.4049\n",
      "====> Epoch: 15 Average Training loss: 104.5785\n",
      "====> Average Test set loss: 104.5679\n",
      "====> Epoch: 16 Average Training loss: 104.4296\n",
      "====> Average Test set loss: 104.1228\n",
      "====> Epoch: 17 Average Training loss: 104.2816\n",
      "====> Average Test set loss: 104.2645\n",
      "====> Epoch: 18 Average Training loss: 104.2028\n",
      "====> Average Test set loss: 103.9130\n",
      "====> Epoch: 19 Average Training loss: 104.0346\n",
      "====> Average Test set loss: 103.9605\n",
      "====> Epoch: 20 Average Training loss: 103.9119\n",
      "====> Average Test set loss: 103.8509\n",
      "====> Epoch: 21 Average Training loss: 103.8244\n",
      "====> Average Test set loss: 103.5190\n",
      "====> Epoch: 22 Average Training loss: 103.7649\n",
      "====> Average Test set loss: 103.3806\n",
      "====> Epoch: 23 Average Training loss: 103.6202\n",
      "====> Average Test set loss: 103.3345\n",
      "====> Epoch: 24 Average Training loss: 103.5830\n",
      "====> Average Test set loss: 103.4961\n",
      "====> Epoch: 25 Average Training loss: 103.5170\n",
      "====> Average Test set loss: 103.3804\n",
      "====> Epoch: 26 Average Training loss: 103.4100\n",
      "====> Average Test set loss: 103.5039\n",
      "====> Epoch: 27 Average Training loss: 103.3605\n",
      "====> Average Test set loss: 103.6989\n",
      "====> Epoch: 28 Average Training loss: 103.2461\n",
      "====> Average Test set loss: 103.0126\n",
      "====> Epoch: 29 Average Training loss: 103.2110\n",
      "====> Average Test set loss: 103.1903\n",
      "====> Epoch: 30 Average Training loss: 103.1448\n",
      "====> Average Test set loss: 103.2399\n",
      "====> Epoch: 31 Average Training loss: 103.1023\n",
      "====> Average Test set loss: 103.0743\n",
      "====> Epoch: 32 Average Training loss: 103.0415\n",
      "====> Average Test set loss: 102.8901\n",
      "====> Epoch: 33 Average Training loss: 102.9848\n",
      "====> Average Test set loss: 103.1117\n",
      "====> Epoch: 34 Average Training loss: 102.9366\n",
      "====> Average Test set loss: 102.9011\n",
      "====> Epoch: 35 Average Training loss: 102.8229\n",
      "====> Average Test set loss: 102.7861\n",
      "====> Epoch: 36 Average Training loss: 102.8012\n",
      "====> Average Test set loss: 102.9908\n",
      "====> Epoch: 37 Average Training loss: 102.7430\n",
      "====> Average Test set loss: 102.8945\n",
      "====> Epoch: 38 Average Training loss: 102.7099\n",
      "====> Average Test set loss: 102.8883\n",
      "====> Epoch: 39 Average Training loss: 102.6951\n",
      "====> Average Test set loss: 102.5686\n",
      "====> Epoch: 40 Average Training loss: 102.6104\n",
      "====> Average Test set loss: 102.8201\n",
      "====> Epoch: 41 Average Training loss: 102.5950\n",
      "====> Average Test set loss: 102.4987\n",
      "====> Epoch: 42 Average Training loss: 102.4932\n",
      "====> Average Test set loss: 102.6231\n",
      "====> Epoch: 43 Average Training loss: 102.4955\n",
      "====> Average Test set loss: 102.7061\n",
      "====> Epoch: 44 Average Training loss: 102.4450\n",
      "====> Average Test set loss: 102.4342\n",
      "====> Epoch: 45 Average Training loss: 102.4172\n",
      "====> Average Test set loss: 102.5133\n",
      "====> Epoch: 46 Average Training loss: 102.3795\n",
      "====> Average Test set loss: 102.3823\n",
      "====> Epoch: 47 Average Training loss: 102.3809\n",
      "====> Average Test set loss: 102.8545\n",
      "====> Epoch: 48 Average Training loss: 102.3086\n",
      "====> Average Test set loss: 102.4429\n",
      "====> Epoch: 49 Average Training loss: 102.3202\n",
      "====> Average Test set loss: 102.4126\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 50):\n",
    "  average_train_loss_per_epoch = VAE_Vanilla_train()\n",
    "  print('====> Epoch: {} Average Training loss: {:.4f}'.format(epoch, average_train_loss_per_epoch))\n",
    "\n",
    "  average_test_loss_per_epoch = VAE_Vanilla_test()\n",
    "  print('====> Average Test set loss: {:.4f}'.format(average_test_loss_per_epoch))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    sample = torch.randn(4, 30).to(device)\n",
    "    sample = model.decoder(sample).cpu()\n",
    "    save_image(sample.view(4, 1, 28, 28),'./results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "2xdWnxNEbyxe",
    "outputId": "490eedd3-d807-4166-c36b-ddc3c2966e7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_10.png  sample_1.png   sample_29.png  sample_38.png  sample_47.png\n",
      "sample_11.png  sample_20.png  sample_2.png   sample_39.png  sample_48.png\n",
      "sample_12.png  sample_21.png  sample_30.png  sample_3.png   sample_49.png\n",
      "sample_13.png  sample_22.png  sample_31.png  sample_40.png  sample_4.png\n",
      "sample_14.png  sample_23.png  sample_32.png  sample_41.png  sample_5.png\n",
      "sample_15.png  sample_24.png  sample_33.png  sample_42.png  sample_6.png\n",
      "sample_16.png  sample_25.png  sample_34.png  sample_43.png  sample_7.png\n",
      "sample_17.png  sample_26.png  sample_35.png  sample_44.png  sample_8.png\n",
      "sample_18.png  sample_27.png  sample_36.png  sample_45.png  sample_9.png\n",
      "sample_19.png  sample_28.png  sample_37.png  sample_46.png\n"
     ]
    }
   ],
   "source": [
    "!ls results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "SB2LP66GcT0F",
    "outputId": "b39ed17c-7339-4f9a-9607-98b72cb078e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAB6CAYAAACr63iqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGKBJREFUeJztnXusXVWdxz8/yxuUV6GUtkCRChbC\noyJSeYgFpYBSRWJQMxSHSEycjExMBhgS0YgGMpMZIDBMQBwLIk+dgRBgKEWCgFDKq8XyaHkXWwry\nFJTnmj/O/q37PXTvnnPPPffccza/T9L0d9Z57LX2Wnvd9Xus37KUEkEQBEF9+MhYVyAIgiDoLjGx\nB0EQ1IyY2IMgCGpGTOxBEAQ1Iyb2IAiCmhETexAEQc2IiT0IgqBmjGhiN7PZZvaomS03s5O7Vakg\nCIKgc6zTDUpmNg54DPgCsAK4B/hGSmlp96oXBEEQDJd1RvDdfYDlKaUnAMzscmAOUDmxm1lscw2C\nIBg+L6aUtmr3wyMxxUwCnpXXK4qyJszsBDNbZGaLRnCtIAiCDzNPD+fDI1mxt0VK6QLgAogVexAE\nQS8YyYr9OWCKvJ5clAVBEARjyEgm9nuAaWY21czWA44Bru1OtYIgCIJO6dgUk1J618z+Afg/YBzw\ni5TSH7tWsyAIgqAjOg537OhiYWMPgiDohHtTSnu3++HYeRoEQVAzYmIPgiCoGTGxB0EQ1IyY2IMg\nCGrGqG9QCgIza/mZOFQ9CLpHrNiDIAhqRkzsQRAENSNMMcGo8ZGPNNYNG220US57//33s/y3v/0t\ny3U1xagZqq5tDPqPWLEHQRDUjFixB11FV6gbbLABAOuuu24ue++997LsK3oYWs0O8qq2bHU+yO0Z\nBNpxzI8bN26Nsqp+KSvXskHpz1ixB0EQ1IyY2IMgCGpGbU0xqqJVqWtePpzPtsO7774LNKttw3Gi\nDYq656yzztAw2nDDDbM8aVLjQK0999wzlz377NChW4sXL87yG2+8AQxe25Wq/u7k+0qr8dnquuqw\nrgtuxlMzi469TTbZJMtbbLEFAJtuumku++tf/5plf14BXnrpJQBef/31XPbWW29l+Z133smy39d+\nHLOxYg+CIKgZMbEHQRDUjFqYYsrUT4240KgMj9SAofjqKrVZP7vxxhsD8Pbbb+cyVeHefPPNLLsa\nVxb1Ac2RIa7O6W+p6qyf7SfWW2+9LO+8885Z/u53v5vlmTNnArD11lvnslWrVmX5wgsvzPKVV14J\nwKuvvprLBs2EoOPITQTtRF+4rCYtNTHo7/pnqu6NjnX/npoPdPyWjcN+o9WzOXXq1Fy2//77Z/nT\nn/50ln38VT1j2he+t2LZsmW5bMmSJVletGhRll9++WWg+dnvl/sYK/YgCIKaERN7EARBzRgIU4yr\nY1Xef5Xd/KFmlM033zzLahZwb7n+rn5v2223zbKbEHQb/Isvvphl3Tbv11O1V2VVjddff32gWS1W\nc8Rrr72W5bHyvuv93WyzzQA488wzc9mRRx6ZZb0PjqqnHikD8JnPfCbLd9xxBwB/+ctfSr/XT1SN\nPY3KcFOVRl8oZWNZzS/6vpsBAcaPH7/GtbQOEyZMyPJuu+22xu9ef/31WX7yySez7FFJY2n6K3vO\nW6HPq6L3zKNhtG0f+9jHsqz356Mf/SgABx54YC7TZ3DhwoVZ/tWvfgU0m2f8PsLwxq/WoRt90HLF\nbma/MLPVZvaQlG1hZvPNbFnx/+Zr+40gCIKgd7SzYv8lcC5wsZSdDCxIKZ1hZicXr0/qfvWaUWdk\nVbk7j3wlDM0r9i233DLLvjLWv5Dbb799ljWW9emnnwaaV+nqjFHHV1kcu75f5hDSlZ3G4OpqoZdo\nHXWVffXVVwMwY8aM0u+98sorWfYV4VNPPZXLtF+0nYceeijQ3N7nnnsuy/3kRFbHsa6QdZz5au2F\nF17IZRoPXabB6XjRa+jqfIcddgBgn332yWWueQIccMABWfZV6Z133pnLFixYsEYdobO4+27Taju/\n4vdPtQ4dI3rPvF+22Wab0vd1THp/Tps2LZdNmTIly3qvfXwuX748lw0nsV1VQEU3aLliTyndBrz0\ngeI5wLxCngd8pau1CoIgCDqmUxv7hJTSykJeBUyo+qCZnQCc0OF1giAIgmEyYudpSimZWaXOkVK6\nALgAYG2fWxtlW/+rHKmuXqpKpCYV3zIMQ+YPdbTo9/R33USgpgJVp4eDqoHuLKxyrvbSYartVZPU\nueeem2U3wWi9VB3+6U9/mmV3NOk9dYcewGGHHZblWbNmAfDnP/85l11zzTVZ7qf4dlXpv/zlL2f5\n+eefz/Lq1auBZvOLtkH7u2xretVW+d133x2Aww8/PJep2UadiQ8++CAAP/vZz3KZxmf341b4dnHT\nhTrbn3jiiSxrX2y11VZAsxl2u+22y7KaWvx31fyo91+fTZ8/qvYctMr2OZq5+jsNd3zezCYCFP+v\n7l6VgiAIgpHQ6cR+LTC3kOcC16zls0EQBEEPaWmKMbPLgIOA8Wa2AjgNOAO40syOB54Gvj6alWzl\nsVePsssasbJy5cosq8rjETSqyqrZRlWsssgFrVfVdmVHI3dUPffPjpX5BYbaoSaGn/zkJ1nWeHOv\n+4033pjLTj311Cxr9ka/J7rNXSNdNJbY9xccdNBBuUxNODfffHOW3VwzVqYE7avHHnssy9rHHgWl\nY6iVuU0/q1EZxx13XJaPOuoooPneqSnsjDPOyPIVV1wBNI+3QTa/KN4Ofe70nuqc4CYTLVNTy6c+\n9aksf/zjHweaTVo6J9x0001Zvu2229Zax7L9NVXx6tqObtByYk8pfaPirYO7WpMgCIKgK0RKgSAI\ngpoxUCkF2gnidxVN1aCyLfwwtNVYTRCajF8/6yYENSvopiLP9KZ1UNQ8U6aGj+W5ip4G4Oijj85l\nu+66a5Y18sA3KJ1yyim5rFXaA227Roaoiuumi9mzZ+eygw8eUgr9ugA/+tGP2rruaKERF7qFXMeO\nZx1U1Vs3qansUVm66ei0007LskYS+ZjUsXfRRRdl2be5Q39t6hot9HnUTI+f/exns+xpMNSkuMce\ne2RZ04z4PdOMjffcc0+WzznnnCyvWLGi6TtQbn6Bof5WE49u5us2sWIPgiCoGQO1YldardDKcmND\n87ZvP7JNk335lm1oXmk+88wzADz++OO5TJ1Wuvr0v+BVq3D9C1+mYfQCvd7EiRMB+MQnPpHLdCV6\n7733ZvmkkxqZI7S9w1ktV6UU8FWrOgW1jrp6//Wvfw3A/fffn8u67XxaGzoudOWs5TvttBPQHC+t\nK3p1DH/uc58D4Gtf+1oumz59epZ1lef35OGHH85ll1xySZY/DKt0GFoNawz62WefnWWP94eh4AjV\nkjRgQlfWPq51P4WfFQDwpz/9Kcveh1WpGcqCKFRbH01ixR4EQVAzYmIPgiCoGQNhiinLllhFWfoB\ndbBodjZ3mqpapk5QdfR5lj5VrfV76mD0eOV2jkVbW9loouqnb5lWlV/jzefNm5dlb+dw6qvmr+98\n5ztZ3mWXXbLs90zvo9ZHzWluzmm1fXu00N8vO7Uehkwie+21V+lnNY7aZXXiKWpecfVfMzZWZQ6t\nS8x6Gf5MVzlEdcyVURVc4eNQTV1uhoVmk1+ZyVWfqzLzqpoa1dzZbWLFHgRBUDNiYg+CIKgZA2GK\nGalKqeqTRnP4kVa65VpljXhwFUuzHmo0zY477rjG9zQWtp/VYq+nRgKoyUkjOIaDm73OOuusXKan\nx2s2vj/84Q9As3lGTRN6wImbHtQEURUN0kuzjKreHj3lkVfQfCiHjkP/rP6WRnVoJJGnMNDoDM9e\nCM1mBe83Hf/9PA5boffXzaCagkKzMCpuItOUDnr/9Tn1Z0BTY/hxedB8r908W/Wcl5kKe5WZNFbs\nQRAENSMm9iAIgpoxEKaY4eCqkKo8qoqqSu9b0lV91c+q7KYYVed0S7tGcHi0jUaWdGrOGC30/ngk\niqr8O++8c5ZPPPHELP/whz8EqjNmfvKTn8yyb5zRlA16Ty688MIsP/LII8DQZh2ojijxaKV+OKdT\nUXOQq/R33HFHLtODHvwgDhhS7++7775cppvCNFLrrrvuAprNUDom1VTo90cPl9F+G+sDS7qBtk0j\nqtSM6ucVL168OJfpPdWNef4M6POskXQaCeeRWho1o2gfeRSPRs1opFe3N9jFij0IgqBm1G7F7ugq\nUlcmunIu296rq66yuGRN3KNOE9+WD0M5ndVBo6uqftj2rffHNY9Vq1blMnVEaWIuTw6mq0td6asz\ny1eo+rvnn39+lm+99dY16qO/pStc7Tdf4erp8GOZRM0pS/TmiaKg2Tldli9cP3v77bdnWVelZXs6\ndGU4efLkLB9xxBFrXFfzieuqdTh7RfoJfQY1xYfuQXHtR3PnH3LIIVnWNCJlx2VqkISv/mFoFa7P\niq68tdzTSWhwxmhqnLFiD4IgqBkxsQdBENSM2ppilCqzTFlsaSuHkqrQVSfQuxpXFVfbb7iZ6O67\n785lqr7r9ndX9fU+6NZoNW+5o/qGG27IZXqcmKqt7qDSPNoay61xxa4Cq3mr30wI7iRTZ5neR88R\nDkP3T01Wan5p5XjXMavO0d///vdA81Z7dRRqLLybgUbriEa9D506bdV04fdSTU8azLBs2bIsu0lU\nnfF63kCZU1tNLppFVB20bpZVs43eP83m6bHwWi91dKtZsRu0XLGb2RQz+52ZLTWzP5rZ94vyLcxs\nvpktK/5fe3KGIAiCoCe0Y4p5F/hBSmk6sC/wPTObDpwMLEgpTQMWFK+DIAiCMaadw6xXAisL+XUz\nexiYBMwBDio+Ng+4FThpVGo5Qqq8z2XH0pW9X/Ub+r5mkPQohW6on73A66Yq5+mnn57lb3/721n2\ntmkEgh4Vp3Hqrn4uWbIkl/kxfDAUPQRw1FFHAXDAAQfkMlVx1TTkqQjUFNMPlGUUVTVfTSJqFnAz\n1MKFC3OZHp6h97fVOFJTgJt21OwwZ86cLD/44INZ9gglNc900xTTjfGvB7N89atfBZqzO+ozqFkU\n/dASTQ1QlWXRI2d0H4GaSdQsVpadUaO6dMy6iUxNmLqnptsMy8ZuZjsAewF3AxOKSR9gFTCh4jsn\nACd0XsUgCIJgOLQdFWNmmwC/AU5MKb2m76XGn/bSP+8ppQtSSnunlPYeUU2DIAiCtmhrxW5m69KY\n1C9NKf22KH7ezCamlFaa2URgdfUvDJ+yAwOGE9BfdQiDbuP94O9/UC77TNUp5Jq5z7cdV21G6LcI\nDq+PRhVcddVVWZ4/f36WPTujevT1UAPdlOVRGRoxpFkw99tvvyzPmjULqFaR1TThkQX9eh9hqO81\ndYBGuujYmTp1KtCcjmHGjBlZvv7667Ps5hM1z2i/6fj2jVwzZ87MZZpdU/vCTWgaVdMPY1bvk54D\n6+fDlqVQABg/fvwa5bqZScekZxYFuPjiiwFYunRpLtNImLIIuqqDV1rNK6Npnm0nKsaAi4CHU0r/\nLm9dC8wt5LnANd2vXhAEQTBc2lmx7wf8HbDEzB4oyv4FOAO40syOB54Gvt7NipU5LqtW7GWraE3i\no04VdV64rAmDWl1D39drKO5s6WeHaRlVKw91AvkqXFd2uuVaHbB+rzSJkq66pk2blmVfqWtf6Onw\nl156aZa7nTBpNPCxpav0BQsWZFkdZx5D7ikAoDme//Of/3yWPQ2AJrS67LLLsqwOZXcwHnjggblM\nY6vLzinoh9QMiqZL0Nz27nhXZ3xVmgUfU6o9aXK2c889N8tPPfUU0HkOe51fyr6n84dq9N2mnaiY\n24EqG8jB3a1OEARBMFIipUAQBEHN6NuUAmXmlarT2HXrvqv0utVY41d127bKTtkp5FqHKqesqtxe\nH4157Qe1tlPUpOQqu8ZAa9Y8dX66Y0sdy3q0mN4TP2bsgQceyGXnnXdeltVBOAh429Sk5W2E5syW\n7rxTB/Gxxx6b5S9+8YtZdse8mlTmzp2bZb2eO0c1h7iO0+uuuy7Lvjegn8fpo48+mmW/l/rsa93V\nOXrnnXcCcMUVV+SyW265Jct6T0ZqPm11/9QZPJpmr1ixB0EQ1IyY2IMgCGqG9VL1MrO2L6YRJy6r\n6UPNKxpHvd122wHN3nKN6tCYVI9M0HjpqoM4WkViqAnC1Tk1H4xW1rx+Q1VN316tUTNTpkzJskbL\neDSHxsFrtsl+O1pwtNF4dE2t4KYsHfN6fzWFgWfH3HrrrXOZRoNofLwf79ZvkVxVe0W++c1vAs2R\nRHqfdO/FvHnzgOYj7MYqM2ir9CZr4d7hbPKMFXsQBEHNiIk9CIKgZvStKabsZHBVx93kAs2qpkdi\naPZBTW6vKpibV8rKoHyzQVkGP30fhswuVQd41NkUo5Rt6qpK76CRG46ayD4s96xd2rmn/gxp5Ihm\nbxw086A+b35QyTbbbJPLtO2+0QiGzHv9cNawMsyUDWGKCYIg+DDTtyt2xf8SlzlUAXbZZZc1vqM5\nlHWbuq7IXa7aBlyW1kCdOVUrJY+319/VFXs/H+k2VrTKdx90Tj8k8+o2ZUcPDueIywEkVuxBEAQf\nZmJiD4IgqBkDYYpxPG81NGdqK1M11fzSaRvLzANVcaiqEroJpobqYBAEY0OYYoIgCD7MxMQeBEFQ\nM/o2u2MZGpveCzpNsB8EQTCWxIo9CIKgZsTEHgRBUDPaOcx6AzNbaGYPmtkfzezHRflUM7vbzJab\n2RVmtuae8CAIgqDntLNifwuYlVLaA9gTmG1m+wJnAv+RUtoJeBk4fvSqGQRBELRLy4k9NfAk5usW\n/xIwC7i6KJ8HfGVUahgEQRAMi7Zs7GY2zsweAFYD84HHgVdSSp54ZQUwaXSqGARBEAyHtib2lNJ7\nKaU9gcnAPsCaWbcqMLMTzGyRmS3qsI5BEATBMBhWVExK6RXgd8BMYDMz8zj4ycBzFd+5IKW093C2\nwwZBEASd005UzFZmtlkhbwh8AXiYxgR/dPGxucA1o1XJIAiCoH3a2Xk6EZhnZuNo/CG4MqV0nZkt\nBS43s9OB+4GLRrGeQRAEQZv0OrvjC8AbwIs9u2hvGU+0bRCJtg0mH6a2bZ9S2qrdL/d0Ygcws0V1\ntbdH2waTaNtgEm2rJlIKBEEQ1IyY2IMgCGrGWEzsF4zBNXtFtG0wibYNJtG2CnpuYw+CIAhGlzDF\nBEEQ1IyY2IMgCGpGTyd2M5ttZo8WOdxP7uW1u42ZTTGz35nZ0iJP/feL8i3MbL6ZLSv+33ys69oJ\nReK3+83suuJ1LfLvm9lmZna1mT1iZg+b2cwa9dk/FWPxITO7rDhLYSD7zcx+YWarzewhKSvtJ2tw\nTtHGxWY2Y+xq3pqKtv1rMSYXm9n/+G7/4r1TirY9amaHtnONnk3sxc7V84DDgOnAN8xseq+uPwq8\nC/wgpTQd2Bf4XtGek4EFKaVpwILi9SDyfRqpI5y65N8/G7gxpbQLsAeNNg58n5nZJOAfgb1TSrsB\n44BjGNx++yUw+wNlVf10GDCt+HcCcH6P6tgpv2TNts0Hdksp7Q48BpwCUMwpxwC7Ft/5z2IuXSu9\nXLHvAyxPKT2RUnobuByY08Prd5WU0sqU0n2F/DqNCWISjTbNKz42kHnqzWwycATw8+K1UYP8+2a2\nKXAgRfqLlNLbRWK7ge+zgnWADYvkfBsBKxnQfksp3Qa89IHiqn6aA1xcnB1xF40EhRN7U9PhU9a2\nlNJNkgb9LhqJFaHRtstTSm+llJ4EltOYS9dKLyf2ScCz8ro2OdzNbAdgL+BuYEJKaWXx1ipgwhhV\nayScBfwz8H7xekvqkX9/KvAC8N+FmennZrYxNeizlNJzwL8Bz9CY0F8F7qUe/eZU9VPd5pa/B24o\n5I7aFs7TEWJmmwC/AU5MKb2m76VGLOlAxZOa2ZeA1Smle8e6LqPAOsAM4PyU0l408hY1mV0Gsc8A\nCnvzHBp/vLYFNmZNdb82DGo/tcLMTqVh5r10JL/Ty4n9OWCKvK7M4T4omNm6NCb1S1NKvy2Kn3c1\nsPh/9VjVr0P2A440s6domMtm0bBLt5V/v89ZAaxIKd1dvL6axkQ/6H0GcAjwZErphZTSO8BvafRl\nHfrNqeqnWswtZnYc8CXgW2log1FHbevlxH4PMK3w0q9HwyFwbQ+v31UKu/NFwMMppX+Xt66lkZ8e\nBjBPfUrplJTS5JTSDjT66JaU0reoQf79lNIq4Fkz27koOhhYyoD3WcEzwL5mtlExNr1tA99vQlU/\nXQscW0TH7Au8KiabgcDMZtMwfx6ZUnpT3roWOMbM1jezqTQcxAtb/mBKqWf/gMNpeHwfB07t5bVH\noS3701AFFwMPFP8Op2GPXgAsA24Gthjruo6gjQcB1xXyjsWAWg5cBaw/1vXrsE17AouKfvtfYPO6\n9BnwY+AR4CHgEmD9Qe034DIavoJ3aGhax1f1E2A0Iu4eB5bQiAwa8zYMs23LadjSfS75L/n8qUXb\nHgUOa+cakVIgCIKgZoTzNAiCoGbExB4EQVAzYmIPgiCoGTGxB0EQ1IyY2IMgCGpGTOxBEAQ1Iyb2\nIAiCmvH/4S/yuS0HspEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the saved image file\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "img=mpimg.imread('results/sample_45.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "muGQm5ARcd79"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Vanilla VAE.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
